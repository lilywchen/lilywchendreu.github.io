---
layout: post
title: Week 2
---
My second week of digging into LLM related research...
I got the data of abstracts from Byron's trialstreamer source, but am running into some extremely irritating compute limitations. To run these extremely large language models, of up to 10 billion parameters, I need to have access to High-RAM GPUs
Currently I have access to two sources:
1. MIT Satori
2. Google Colab Free

MIT Satori has environment and compatibility issues that don't allow for transformers module use
Google Colab Free doesn't have enough RAM to run inference on large models without timing out and erasing everything

Hopefully compute gets resolved soon...
